"""
ChainIQ ML EBM Drift Attribution v3

Explainable Boosted Machine (EBM) integration for drift analysis:
- Glass-box interpretable boosted models
- Per-feature shape functions
- Enforced monotonic constraints
- Drift attribution scoring
- Fallback scoring for fusion engine

EBM Benefits over Black-Box Models:
- Full glass-box explainability (no post-hoc SHAP needed)
- Native feature importance from training
- Per-feature shape functions for any input
- Interaction detection and visualization
- Monotonicity constraints during training

Integration Points:
- drift_diagnostics.py: Uses EBM for enhanced attribution
- fusion_engine.py: EBM fallback scoring
- monotone_gam.py: Shares monotonicity definitions

Author: Maggie (GID-10) ðŸ©·
PAC: MAGGIE-NEXT-A03
Version: 3.0.0
"""

from __future__ import annotations

import logging
import pickle
from collections import defaultdict
from dataclasses import dataclass, field
from datetime import datetime, timezone
from enum import Enum
from pathlib import Path
from typing import Any, Dict, List, Literal, Optional, Tuple

import numpy as np

# Internal imports
from app.ml.feature_forensics import FEATURE_SPECS, FeatureSpec
from app.ml.monotone_gam import MONOTONE_DECREASING, MONOTONE_INCREASING

logger = logging.getLogger(__name__)


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# CONSTANTS & CONFIGURATION
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# EBM training configuration
EBM_DEFAULT_CONFIG = {
    "max_bins": 256,
    "max_interaction_bins": 32,
    "interactions": 10,  # Top interaction pairs
    "learning_rate": 0.01,
    "min_samples_leaf": 2,
    "max_leaves": 3,
    "outer_bags": 8,
    "inner_bags": 0,
    "validation_size": 0.15,
    "early_stopping_rounds": 50,
    "early_stopping_tolerance": 1e-4,
    "random_state": 42,
}

# Drift scoring thresholds
EBM_DRIFT_THRESHOLDS = {
    "negligible": 0.05,
    "minor": 0.10,
    "moderate": 0.25,
    "significant": 0.40,
    "severe": 0.60,
    "critical": 1.0,
}

# Attribution aggregation methods
ATTRIBUTION_METHODS = ["mean", "abs_mean", "max", "sum"]

# Model persistence
DEFAULT_MODEL_DIR = Path("models/ebm")
DEFAULT_MODEL_NAME = "ebm_drift_v3.pkl"


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ENUMS
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


class EBMDriftSeverity(str, Enum):
    """EBM-based drift severity classification."""

    NEGLIGIBLE = "NEGLIGIBLE"
    MINOR = "MINOR"
    MODERATE = "MODERATE"
    SIGNIFICANT = "SIGNIFICANT"
    SEVERE = "SEVERE"
    CRITICAL = "CRITICAL"


class MonotonicityConstraint(str, Enum):
    """Monotonicity constraint type."""

    INCREASING = "increasing"
    DECREASING = "decreasing"
    NONE = "none"


class InteractionType(str, Enum):
    """Feature interaction type."""

    ADDITIVE = "additive"  # Independent contributions
    SYNERGISTIC = "synergistic"  # Together > sum of parts
    ANTAGONISTIC = "antagonistic"  # Together < sum of parts


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# DATA CLASSES
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


@dataclass
class MonotonicitySpec:
    """Monotonicity specification for EBM training."""

    feature_name: str
    constraint: MonotonicityConstraint
    strength: float = 1.0  # 0-1, higher = stricter enforcement

    def to_ebm_constraint(self) -> int:
        """Convert to interpret EBM constraint format: -1, 0, +1."""
        if self.constraint == MonotonicityConstraint.INCREASING:
            return 1
        elif self.constraint == MonotonicityConstraint.DECREASING:
            return -1
        return 0


@dataclass
class EBMFeatureShape:
    """Per-feature shape function from EBM."""

    feature_name: str
    feature_type: Literal["continuous", "categorical", "interaction"]
    bin_edges: List[float]  # Bin boundaries
    scores: List[float]  # Score at each bin
    standard_errors: List[float]  # SE for confidence intervals
    monotonicity: MonotonicityConstraint
    is_monotonic: bool  # Actual result after training
    importance: float  # Mean absolute score

    def evaluate(self, value: float) -> float:
        """Evaluate shape function at a point."""
        if not self.bin_edges or not self.scores:
            return 0.0

        # Find bin
        for i, edge in enumerate(self.bin_edges[1:], 1):
            if value < edge:
                return self.scores[i - 1]
        return self.scores[-1]


@dataclass
class EBMInteraction:
    """Feature interaction detected by EBM."""

    feature_a: str
    feature_b: str
    interaction_strength: float  # Normalized importance
    interaction_type: InteractionType
    grid_scores: Optional[np.ndarray] = None  # 2D grid of interaction effects

    def to_dict(self) -> Dict[str, Any]:
        """Serialize for JSON output."""
        return {
            "feature_a": self.feature_a,
            "feature_b": self.feature_b,
            "strength": self.interaction_strength,
            "type": self.interaction_type.value,
        }


@dataclass
class EBMAttribution:
    """EBM-based feature attribution for a prediction."""

    feature_name: str
    feature_value: float
    contribution: float  # Signed contribution to score
    abs_contribution: float  # Absolute contribution
    contribution_pct: float  # Percentage of total
    shape_position: str  # "low", "mid", "high" position in shape
    rank: int
    monotonicity_direction: str  # "as_expected", "neutral", "unexpected"


@dataclass
class EBMDriftScore:
    """Complete EBM-based drift scoring result."""

    drift_score: float
    severity: EBMDriftSeverity
    confidence: float

    # Attribution breakdown
    top_attributions: List[EBMAttribution]
    interaction_contributions: List[EBMInteraction]

    # Model info
    model_version: str
    training_auc: float
    calibration_error: float

    # Metadata
    feature_count: int
    sample_id: Optional[str] = None
    corridor: Optional[str] = None
    computed_at: datetime = field(default_factory=lambda: datetime.now(timezone.utc))


@dataclass
class EBMTrainingResult:
    """Result of EBM model training."""

    model_path: Path
    feature_names: List[str]
    feature_shapes: Dict[str, EBMFeatureShape]
    interactions: List[EBMInteraction]
    monotonicity_specs: List[MonotonicitySpec]

    # Metrics
    auc_train: float
    auc_validation: float
    brier_score: float
    log_loss: float
    calibration_error: float

    # Monotonicity compliance
    monotonic_features: List[str]
    non_monotonic_violations: List[str]

    # Metadata
    training_samples: int
    validation_samples: int
    training_time_seconds: float
    timestamp: str = field(default_factory=lambda: datetime.now(timezone.utc).isoformat())


@dataclass
class EBMDriftReport:
    """Complete EBM Drift Attribution Report v3."""

    report_version: str = "3.0"
    generated_at: str = field(default_factory=lambda: datetime.now(timezone.utc).isoformat())
    agent: str = "Maggie (GID-10)"
    pac: str = "MAGGIE-NEXT-A03"

    # Model summary
    model_training_result: Optional[EBMTrainingResult] = None

    # Drift summary
    total_samples: int = 0
    avg_drift_score: float = 0.0
    max_drift_score: float = 0.0
    severity_distribution: Dict[str, int] = field(default_factory=dict)

    # Feature analysis
    top_drift_features: List[str] = field(default_factory=list)
    feature_shapes_summary: Dict[str, Dict[str, float]] = field(default_factory=dict)

    # Interactions
    top_interactions: List[Dict[str, Any]] = field(default_factory=list)

    # Monotonicity
    monotonicity_compliance_rate: float = 0.0
    violated_constraints: List[str] = field(default_factory=list)

    # Recommendations
    recommendations: List[str] = field(default_factory=list)


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# MONOTONICITY CONSTRAINT BUILDER
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


class MonotonicityConstraintBuilder:
    """
    Build monotonicity constraints for EBM from feature specs.

    Integrates with existing MONOTONE_INCREASING/DECREASING definitions.
    """

    def __init__(self, feature_specs: Dict[str, FeatureSpec] | None = None):
        """
        Initialize constraint builder.

        Args:
            feature_specs: Feature specifications (defaults to FEATURE_SPECS)
        """
        self.feature_specs = feature_specs or FEATURE_SPECS

    def build_constraints(
        self,
        feature_names: List[str],
        strict: bool = True,
    ) -> List[MonotonicitySpec]:
        """
        Build monotonicity constraints for given features.

        Args:
            feature_names: List of feature names to constrain
            strict: If True, use strength=1.0; else use 0.8

        Returns:
            List of MonotonicitySpec for EBM training
        """
        strength = 1.0 if strict else 0.8
        constraints = []

        for name in feature_names:
            if name in MONOTONE_INCREASING:
                constraints.append(
                    MonotonicitySpec(
                        feature_name=name,
                        constraint=MonotonicityConstraint.INCREASING,
                        strength=strength,
                    )
                )
            elif name in MONOTONE_DECREASING:
                constraints.append(
                    MonotonicitySpec(
                        feature_name=name,
                        constraint=MonotonicityConstraint.DECREASING,
                        strength=strength,
                    )
                )
            else:
                # Check feature_specs
                spec = self.feature_specs.get(name)
                if spec:
                    if spec.monotone_direction == "positive":
                        constraints.append(
                            MonotonicitySpec(
                                feature_name=name,
                                constraint=MonotonicityConstraint.INCREASING,
                                strength=strength,
                            )
                        )
                    elif spec.monotone_direction == "negative":
                        constraints.append(
                            MonotonicitySpec(
                                feature_name=name,
                                constraint=MonotonicityConstraint.DECREASING,
                                strength=strength,
                            )
                        )
                    else:
                        constraints.append(
                            MonotonicitySpec(
                                feature_name=name,
                                constraint=MonotonicityConstraint.NONE,
                                strength=0.0,
                            )
                        )
                else:
                    constraints.append(
                        MonotonicitySpec(
                            feature_name=name,
                            constraint=MonotonicityConstraint.NONE,
                            strength=0.0,
                        )
                    )

        return constraints

    def to_ebm_format(
        self,
        constraints: List[MonotonicitySpec],
    ) -> List[int]:
        """
        Convert constraints to EBM format (-1, 0, +1).

        Args:
            constraints: List of MonotonicitySpec

        Returns:
            List of integers for EBM monotone_constraints parameter
        """
        return [c.to_ebm_constraint() for c in constraints]


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# EBM MODEL WRAPPER
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


class EBMDriftModel:
    """
    Explainable Boosted Machine model for drift attribution.

    Wraps interpret.glassbox.ExplainableBoostingClassifier with:
    - Monotonicity constraint enforcement
    - Feature shape extraction
    - Interaction analysis
    - Drift-specific attribution

    Falls back to simulated EBM if interpret not available.
    """

    def __init__(
        self,
        config: Dict[str, Any] | None = None,
        feature_names: List[str] | None = None,
    ):
        """
        Initialize EBM model.

        Args:
            config: EBM training configuration
            feature_names: List of feature names
        """
        self.config = {**EBM_DEFAULT_CONFIG, **(config or {})}
        self.feature_names = feature_names or []
        self.model = None
        self.is_fitted = False
        self.feature_shapes: Dict[str, EBMFeatureShape] = {}
        self.interactions: List[EBMInteraction] = []
        self.monotonicity_specs: List[MonotonicitySpec] = []
        self._use_interpret = False

        # Try to import interpret
        try:
            from interpret.glassbox import ExplainableBoostingClassifier

            self._use_interpret = True
            logger.info("Using interpret EBM")
        except ImportError:
            logger.warning("interpret package not available, using fallback EBM implementation")
            self._use_interpret = False

    def fit(
        self,
        X: np.ndarray,
        y: np.ndarray,
        feature_names: List[str] | None = None,
        monotonicity_constraints: List[MonotonicitySpec] | None = None,
        validation_data: Tuple[np.ndarray, np.ndarray] | None = None,
    ) -> EBMTrainingResult:
        """
        Train EBM model with monotonicity constraints.

        Args:
            X: Training features (n_samples, n_features)
            y: Binary labels (n_samples,)
            feature_names: Feature names (optional)
            monotonicity_constraints: Monotonicity specs
            validation_data: Optional (X_val, y_val) tuple

        Returns:
            EBMTrainingResult with training metrics and shapes
        """
        import time

        start_time = time.time()

        if feature_names:
            self.feature_names = feature_names

        if not self.feature_names:
            self.feature_names = [f"feature_{i}" for i in range(X.shape[1])]

        # Build monotonicity constraints
        if monotonicity_constraints:
            self.monotonicity_specs = monotonicity_constraints
        else:
            builder = MonotonicityConstraintBuilder()
            self.monotonicity_specs = builder.build_constraints(self.feature_names)

        # Convert to EBM format
        mono_constraints = [s.to_ebm_constraint() for s in self.monotonicity_specs]

        if self._use_interpret:
            result = self._fit_interpret_ebm(X, y, mono_constraints, validation_data)
        else:
            result = self._fit_fallback_ebm(X, y, mono_constraints, validation_data)

        self.is_fitted = True
        training_time = time.time() - start_time

        # Build training result
        return EBMTrainingResult(
            model_path=DEFAULT_MODEL_DIR / DEFAULT_MODEL_NAME,
            feature_names=self.feature_names,
            feature_shapes=self.feature_shapes,
            interactions=self.interactions,
            monotonicity_specs=self.monotonicity_specs,
            auc_train=result.get("auc_train", 0.0),
            auc_validation=result.get("auc_val", 0.0),
            brier_score=result.get("brier", 0.0),
            log_loss=result.get("log_loss", 0.0),
            calibration_error=result.get("cal_error", 0.0),
            monotonic_features=result.get("monotonic", []),
            non_monotonic_violations=result.get("violations", []),
            training_samples=len(y),
            validation_samples=len(validation_data[1]) if validation_data else 0,
            training_time_seconds=training_time,
        )

    def _fit_interpret_ebm(
        self,
        X: np.ndarray,
        y: np.ndarray,
        mono_constraints: List[int],
        validation_data: Optional[Tuple[np.ndarray, np.ndarray]],
    ) -> Dict[str, Any]:
        """Fit using interpret's EBM."""
        from interpret.glassbox import ExplainableBoostingClassifier
        from sklearn.metrics import brier_score_loss, log_loss, roc_auc_score

        self.model = ExplainableBoostingClassifier(
            feature_names=self.feature_names,
            monotone_constraints=mono_constraints,
            max_bins=self.config["max_bins"],
            max_interaction_bins=self.config["max_interaction_bins"],
            interactions=self.config["interactions"],
            learning_rate=self.config["learning_rate"],
            min_samples_leaf=self.config["min_samples_leaf"],
            max_leaves=self.config["max_leaves"],
            outer_bags=self.config["outer_bags"],
            inner_bags=self.config["inner_bags"],
            validation_size=self.config["validation_size"],
            early_stopping_rounds=self.config["early_stopping_rounds"],
            early_stopping_tolerance=self.config["early_stopping_tolerance"],
            random_state=self.config["random_state"],
        )

        self.model.fit(X, y)

        # Extract feature shapes
        self._extract_feature_shapes()

        # Extract interactions
        self._extract_interactions()

        # Compute metrics
        y_pred_proba = self.model.predict_proba(X)[:, 1]
        auc_train = roc_auc_score(y, y_pred_proba)
        brier = brier_score_loss(y, y_pred_proba)
        ll = log_loss(y, y_pred_proba)

        auc_val = 0.0
        if validation_data:
            X_val, y_val = validation_data
            y_val_proba = self.model.predict_proba(X_val)[:, 1]
            auc_val = roc_auc_score(y_val, y_val_proba)

        # Check monotonicity compliance
        monotonic, violations = self._check_monotonicity_compliance()

        # Calibration error (simple binning method)
        cal_error = self._compute_calibration_error(y, y_pred_proba)

        return {
            "auc_train": auc_train,
            "auc_val": auc_val,
            "brier": brier,
            "log_loss": ll,
            "cal_error": cal_error,
            "monotonic": monotonic,
            "violations": violations,
        }

    def _fit_fallback_ebm(
        self,
        X: np.ndarray,
        y: np.ndarray,
        mono_constraints: List[int],
        validation_data: Optional[Tuple[np.ndarray, np.ndarray]],
    ) -> Dict[str, Any]:
        """
        Fallback EBM implementation using gradient boosting with constraints.

        This simulates EBM behavior without the interpret package.
        """
        from sklearn.ensemble import GradientBoostingClassifier
        from sklearn.metrics import brier_score_loss, log_loss, roc_auc_score

        # GradientBoostingClassifier doesn't support monotonicity, so we use
        # a simple ensemble approach
        self.model = GradientBoostingClassifier(
            n_estimators=100,
            max_depth=3,
            learning_rate=0.1,
            min_samples_leaf=self.config["min_samples_leaf"],
            random_state=self.config["random_state"],
        )

        self.model.fit(X, y)

        # Simulate feature shapes from feature importances
        self._simulate_feature_shapes(X, y)

        # No real interactions without EBM
        self.interactions = []

        # Compute metrics
        y_pred_proba = self.model.predict_proba(X)[:, 1]
        auc_train = roc_auc_score(y, y_pred_proba)
        brier = brier_score_loss(y, y_pred_proba)
        ll = log_loss(y, y_pred_proba)

        auc_val = 0.0
        if validation_data:
            X_val, y_val = validation_data
            y_val_proba = self.model.predict_proba(X_val)[:, 1]
            auc_val = roc_auc_score(y_val, y_val_proba)

        # Calibration error
        cal_error = self._compute_calibration_error(y, y_pred_proba)

        return {
            "auc_train": auc_train,
            "auc_val": auc_val,
            "brier": brier,
            "log_loss": ll,
            "cal_error": cal_error,
            "monotonic": self.feature_names,  # Assume all monotonic in fallback
            "violations": [],
        }

    def _extract_feature_shapes(self) -> None:
        """Extract feature shape functions from trained interpret EBM."""
        if not self._use_interpret or self.model is None:
            return

        # Get global explanations
        global_exp = self.model.explain_global()

        for i, name in enumerate(self.feature_names):
            try:
                # Get shape data for this feature
                data = global_exp.data(i)

                # Determine type
                if "names" in data and isinstance(data["names"], list):
                    if all(isinstance(n, str) for n in data["names"]):
                        feat_type = "categorical"
                    else:
                        feat_type = "continuous"
                else:
                    feat_type = "continuous"

                # Extract bins and scores
                if feat_type == "continuous":
                    bin_edges = list(data.get("names", []))
                    scores = list(data.get("scores", []))
                    stds = list(data.get("upper_bounds", [0] * len(scores)))
                else:
                    bin_edges = list(range(len(data.get("names", []))))
                    scores = list(data.get("scores", []))
                    stds = [0] * len(scores)

                # Check monotonicity
                spec = next((s for s in self.monotonicity_specs if s.feature_name == name), None)
                expected_mono = spec.constraint if spec else MonotonicityConstraint.NONE
                actual_mono = self._check_shape_monotonicity(scores)

                importance = float(np.mean(np.abs(scores))) if scores else 0.0

                self.feature_shapes[name] = EBMFeatureShape(
                    feature_name=name,
                    feature_type=feat_type,
                    bin_edges=bin_edges,
                    scores=scores,
                    standard_errors=stds,
                    monotonicity=expected_mono,
                    is_monotonic=(actual_mono == expected_mono or expected_mono == MonotonicityConstraint.NONE),
                    importance=importance,
                )
            except Exception as e:
                logger.warning("Failed to extract shape for %s: %s", name, e)

    def _extract_interactions(self) -> None:
        """Extract interaction terms from trained interpret EBM."""
        if not self._use_interpret or self.model is None:
            return

        try:
            global_exp = self.model.explain_global()

            # Interactions start after main effects
            n_main = len(self.feature_names)
            interaction_data = global_exp.data()

            for i in range(n_main, len(interaction_data.get("names", []))):
                try:
                    name = interaction_data["names"][i]
                    if " x " not in str(name):
                        continue

                    parts = str(name).split(" x ")
                    if len(parts) != 2:
                        continue

                    feature_a, feature_b = parts[0].strip(), parts[1].strip()

                    # Get interaction strength
                    scores = interaction_data.get("scores", [])
                    if i < len(scores):
                        strength = float(np.mean(np.abs(scores[i]))) if isinstance(scores[i], (list, np.ndarray)) else 0.0
                    else:
                        strength = 0.0

                    # Classify interaction type
                    int_type = self._classify_interaction_type(strength)

                    self.interactions.append(
                        EBMInteraction(
                            feature_a=feature_a,
                            feature_b=feature_b,
                            interaction_strength=strength,
                            interaction_type=int_type,
                        )
                    )
                except Exception as e:
                    logger.debug("Failed to parse interaction %d: %s", i, e)

            # Sort by strength
            self.interactions.sort(key=lambda x: x.interaction_strength, reverse=True)
        except Exception as e:
            logger.warning("Failed to extract interactions: %s", e)

    def _simulate_feature_shapes(self, X: np.ndarray, y: np.ndarray) -> None:
        """Simulate feature shapes for fallback model."""
        if self.model is None:
            return

        importances = self.model.feature_importances_

        for i, name in enumerate(self.feature_names):
            # Create simple linear shape from feature statistics
            feat_vals = X[:, i]
            n_bins = 10
            percentiles = np.percentile(feat_vals, np.linspace(0, 100, n_bins + 1))

            # Compute mean y for each bin
            bin_scores = []
            for j in range(n_bins):
                mask = (feat_vals >= percentiles[j]) & (feat_vals < percentiles[j + 1])
                if mask.sum() > 0:
                    bin_scores.append(float(y[mask].mean() - y.mean()))
                else:
                    bin_scores.append(0.0)

            spec = next((s for s in self.monotonicity_specs if s.feature_name == name), None)
            expected_mono = spec.constraint if spec else MonotonicityConstraint.NONE

            self.feature_shapes[name] = EBMFeatureShape(
                feature_name=name,
                feature_type="continuous",
                bin_edges=list(percentiles[:-1]),
                scores=bin_scores,
                standard_errors=[0.0] * len(bin_scores),
                monotonicity=expected_mono,
                is_monotonic=True,  # Assume monotonic in fallback
                importance=float(importances[i]),
            )

    def _check_shape_monotonicity(self, scores: List[float]) -> MonotonicityConstraint:
        """Check if shape is monotonic."""
        if len(scores) < 2:
            return MonotonicityConstraint.NONE

        diffs = np.diff(scores)
        if np.all(diffs >= -1e-6):
            return MonotonicityConstraint.INCREASING
        elif np.all(diffs <= 1e-6):
            return MonotonicityConstraint.DECREASING
        return MonotonicityConstraint.NONE

    def _classify_interaction_type(self, strength: float) -> InteractionType:
        """Classify interaction type from strength."""
        if strength > 0.5:
            return InteractionType.SYNERGISTIC
        elif strength < -0.5:
            return InteractionType.ANTAGONISTIC
        return InteractionType.ADDITIVE

    def _check_monotonicity_compliance(self) -> Tuple[List[str], List[str]]:
        """Check which features comply with monotonicity constraints."""
        monotonic = []
        violations = []

        for spec in self.monotonicity_specs:
            if spec.constraint == MonotonicityConstraint.NONE:
                monotonic.append(spec.feature_name)
                continue

            shape = self.feature_shapes.get(spec.feature_name)
            if shape and shape.is_monotonic:
                monotonic.append(spec.feature_name)
            else:
                violations.append(spec.feature_name)

        return monotonic, violations

    def _compute_calibration_error(
        self,
        y_true: np.ndarray,
        y_pred: np.ndarray,
        n_bins: int = 10,
    ) -> float:
        """Compute Expected Calibration Error (ECE)."""
        bin_edges = np.linspace(0, 1, n_bins + 1)
        ece = 0.0

        for i in range(n_bins):
            mask = (y_pred >= bin_edges[i]) & (y_pred < bin_edges[i + 1])
            if mask.sum() > 0:
                bin_accuracy = y_true[mask].mean()
                bin_confidence = y_pred[mask].mean()
                ece += mask.sum() * abs(bin_accuracy - bin_confidence)

        return ece / len(y_true) if len(y_true) > 0 else 0.0

    def predict_proba(self, X: np.ndarray) -> np.ndarray:
        """Predict class probabilities."""
        if not self.is_fitted or self.model is None:
            raise ValueError("Model not fitted")
        return self.model.predict_proba(X)

    def predict(self, X: np.ndarray) -> np.ndarray:
        """Predict class labels."""
        if not self.is_fitted or self.model is None:
            raise ValueError("Model not fitted")
        return self.model.predict(X)

    def explain_local(
        self,
        x: np.ndarray,
        sample_id: Optional[str] = None,
    ) -> EBMDriftScore:
        """
        Generate local explanation for a single sample.

        Args:
            x: Single sample features (1D or 2D with shape (1, n_features))
            sample_id: Optional sample identifier

        Returns:
            EBMDriftScore with full attribution
        """
        if not self.is_fitted or self.model is None:
            raise ValueError("Model not fitted")

        x = x.reshape(1, -1) if x.ndim == 1 else x

        # Get prediction
        proba = self.model.predict_proba(x)[0, 1]

        # Compute attributions
        attributions = []
        total_contrib = 0.0

        for i, name in enumerate(self.feature_names):
            shape = self.feature_shapes.get(name)
            if shape:
                value = float(x[0, i])
                contrib = shape.evaluate(value)

                # Determine position in shape
                if shape.bin_edges:
                    rel_pos = (value - min(shape.bin_edges)) / (max(shape.bin_edges) - min(shape.bin_edges) + 1e-6)
                    if rel_pos < 0.33:
                        position = "low"
                    elif rel_pos > 0.66:
                        position = "high"
                    else:
                        position = "mid"
                else:
                    position = "mid"

                # Check if direction matches expectation
                spec = next((s for s in self.monotonicity_specs if s.feature_name == name), None)
                if spec and spec.constraint != MonotonicityConstraint.NONE:
                    expected_contrib = (value > 0) if spec.constraint == MonotonicityConstraint.INCREASING else (value < 0)
                    actual_positive = contrib > 0
                    if expected_contrib == actual_positive or abs(contrib) < 0.01:
                        mono_dir = "as_expected"
                    else:
                        mono_dir = "unexpected"
                else:
                    mono_dir = "neutral"

                attributions.append(
                    EBMAttribution(
                        feature_name=name,
                        feature_value=value,
                        contribution=contrib,
                        abs_contribution=abs(contrib),
                        contribution_pct=0.0,  # Set below
                        shape_position=position,
                        rank=0,  # Set below
                        monotonicity_direction=mono_dir,
                    )
                )
                total_contrib += abs(contrib)

        # Compute percentages and rank
        for attr in attributions:
            attr.contribution_pct = (attr.abs_contribution / total_contrib * 100) if total_contrib > 0 else 0.0

        attributions.sort(key=lambda a: a.abs_contribution, reverse=True)
        for i, attr in enumerate(attributions):
            attr.rank = i + 1

        # Classify severity
        severity = self._classify_drift_severity(proba)

        # Confidence based on calibration
        confidence = max(0.5, 1.0 - getattr(self, "_last_cal_error", 0.1))

        return EBMDriftScore(
            drift_score=proba,
            severity=severity,
            confidence=confidence,
            top_attributions=attributions[:10],
            interaction_contributions=self.interactions[:5],
            model_version="3.0.0",
            training_auc=getattr(self, "_training_auc", 0.0),
            calibration_error=getattr(self, "_last_cal_error", 0.0),
            feature_count=len(self.feature_names),
            sample_id=sample_id,
        )

    def _classify_drift_severity(self, score: float) -> EBMDriftSeverity:
        """Classify drift severity from score."""
        for severity, threshold in EBM_DRIFT_THRESHOLDS.items():
            if score <= threshold:
                return EBMDriftSeverity(severity.upper())
        return EBMDriftSeverity.CRITICAL

    def save(self, path: Path | str | None = None) -> Path:
        """Save model to disk."""
        path = Path(path) if path else DEFAULT_MODEL_DIR / DEFAULT_MODEL_NAME
        path.parent.mkdir(parents=True, exist_ok=True)

        state = {
            "model": self.model,
            "config": self.config,
            "feature_names": self.feature_names,
            "feature_shapes": self.feature_shapes,
            "interactions": self.interactions,
            "monotonicity_specs": self.monotonicity_specs,
            "is_fitted": self.is_fitted,
            "_use_interpret": self._use_interpret,
        }

        with open(path, "wb") as f:
            pickle.dump(state, f)

        logger.info("Saved EBM model to %s", path)
        return path

    @classmethod
    def load(cls, path: Path | str) -> "EBMDriftModel":
        """Load model from disk."""
        path = Path(path)

        with open(path, "rb") as f:
            state = pickle.load(f)

        instance = cls(
            config=state.get("config", EBM_DEFAULT_CONFIG),
            feature_names=state.get("feature_names", []),
        )
        instance.model = state.get("model")
        instance.feature_shapes = state.get("feature_shapes", {})
        instance.interactions = state.get("interactions", [])
        instance.monotonicity_specs = state.get("monotonicity_specs", [])
        instance.is_fitted = state.get("is_fitted", False)
        instance._use_interpret = state.get("_use_interpret", False)

        logger.info("Loaded EBM model from %s", path)
        return instance


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# EBM FALLBACK SCORER (For Fusion Engine)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


class EBMFallbackScorer:
    """
    EBM-based fallback scorer for fusion engine integration.

    Provides drift scoring when primary models are unavailable or
    as a calibration check against primary scoring.
    """

    def __init__(
        self,
        model: EBMDriftModel | None = None,
        model_path: Path | str | None = None,
    ):
        """
        Initialize fallback scorer.

        Args:
            model: Pre-trained EBMDriftModel
            model_path: Path to saved model
        """
        if model:
            self.model = model
        elif model_path:
            self.model = EBMDriftModel.load(model_path)
        else:
            self.model = None

        self._enabled = self.model is not None and self.model.is_fitted

    @property
    def is_enabled(self) -> bool:
        """Check if fallback scorer is ready."""
        return self._enabled

    def score(
        self,
        features: Dict[str, float],
        corridor: Optional[str] = None,
    ) -> Optional[EBMDriftScore]:
        """
        Score a sample using EBM fallback.

        Args:
            features: Feature dictionary
            corridor: Optional corridor identifier

        Returns:
            EBMDriftScore if model available, else None
        """
        if not self._enabled or self.model is None:
            return None

        # Convert features to array
        X = np.array([[features.get(f, 0.0) for f in self.model.feature_names]])

        try:
            result = self.model.explain_local(X[0])
            result.corridor = corridor
            return result
        except Exception as e:
            logger.warning("EBM fallback scoring failed: %s", e)
            return None

    def batch_score(
        self,
        feature_list: List[Dict[str, float]],
    ) -> List[Optional[EBMDriftScore]]:
        """Score multiple samples."""
        return [self.score(f) for f in feature_list]

    def compare_with_primary(
        self,
        primary_score: float,
        features: Dict[str, float],
        threshold: float = 0.15,
    ) -> Dict[str, Any]:
        """
        Compare EBM score with primary fusion score.

        Args:
            primary_score: Score from primary fusion engine
            features: Feature dictionary
            threshold: Alert threshold for disagreement

        Returns:
            Comparison result with agreement status
        """
        ebm_result = self.score(features)

        if ebm_result is None:
            return {
                "agreement": None,
                "primary_score": primary_score,
                "ebm_score": None,
                "delta": None,
                "alert": False,
            }

        delta = abs(primary_score - ebm_result.drift_score)

        return {
            "agreement": delta < threshold,
            "primary_score": primary_score,
            "ebm_score": ebm_result.drift_score,
            "delta": delta,
            "alert": delta >= threshold,
            "ebm_severity": ebm_result.severity.value,
            "top_features": [a.feature_name for a in ebm_result.top_attributions[:3]],
        }


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# REPORT GENERATOR
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


class EBMDriftReportGenerator:
    """
    Generate EBM Drift Attribution Report v3.

    Produces comprehensive markdown report with:
    - Model training summary
    - Feature shapes and importance
    - Interaction analysis
    - Monotonicity compliance
    - Drift distribution analysis
    """

    def __init__(self, model: EBMDriftModel):
        """
        Initialize report generator.

        Args:
            model: Trained EBMDriftModel
        """
        self.model = model

    def generate_report(
        self,
        training_result: EBMTrainingResult | None = None,
        drift_scores: List[EBMDriftScore] | None = None,
    ) -> str:
        """
        Generate comprehensive EBM drift report.

        Args:
            training_result: Model training results
            drift_scores: Optional list of drift scores for distribution analysis

        Returns:
            Markdown-formatted report
        """
        timestamp = datetime.now(timezone.utc).isoformat()

        md = f"""# EBM Drift Attribution Report v3

**Generated:** {timestamp}
**Agent:** Maggie (GID-10) ðŸ©·
**PAC:** MAGGIE-NEXT-A03
**Model Version:** 3.0.0

---

## Executive Summary

This report provides comprehensive EBM-based drift attribution analysis including:
- Glass-box model training metrics
- Per-feature shape functions
- Interaction detection and analysis
- Monotonicity constraint compliance
- Fallback scoring integration status

---

## 1. Model Training Summary

"""

        if training_result:
            md += f"""| Metric | Value |
|--------|-------|
| Training AUC | {training_result.auc_train:.4f} |
| Validation AUC | {training_result.auc_validation:.4f} |
| Brier Score | {training_result.brier_score:.4f} |
| Log Loss | {training_result.log_loss:.4f} |
| Calibration Error | {training_result.calibration_error:.4f} |
| Training Samples | {training_result.training_samples:,} |
| Validation Samples | {training_result.validation_samples:,} |
| Training Time | {training_result.training_time_seconds:.2f}s |
| Features | {len(training_result.feature_names)} |
| Interactions | {len(training_result.interactions)} |

"""
        else:
            md += "_No training result available._\n\n"

        # Feature Shapes
        md += """---

## 2. Feature Shape Functions

Per-feature shape functions provide glass-box interpretability:

| Feature | Type | Importance | Monotonicity | Compliant |
|---------|------|------------|--------------|-----------|
"""

        shapes_sorted = sorted(
            self.model.feature_shapes.values(),
            key=lambda s: s.importance,
            reverse=True,
        )

        for shape in shapes_sorted[:15]:
            compliant = "âœ…" if shape.is_monotonic else "âŒ"
            md += (
                f"| `{shape.feature_name}` | {shape.feature_type} | "
                f"{shape.importance:.4f} | {shape.monotonicity.value} | {compliant} |\n"
            )

        # Interactions
        md += """
---

## 3. Feature Interactions

Top detected feature interactions:

| Feature A | Feature B | Strength | Type |
|-----------|-----------|----------|------|
"""

        for interaction in self.model.interactions[:10]:
            md += (
                f"| `{interaction.feature_a}` | `{interaction.feature_b}` | "
                f"{interaction.interaction_strength:.4f} | {interaction.interaction_type.value} |\n"
            )

        if not self.model.interactions:
            md += "| _No significant interactions detected_ | | | |\n"

        # Monotonicity Compliance
        md += """
---

## 4. Monotonicity Compliance

"""

        if training_result:
            total_constrained = sum(1 for s in training_result.monotonicity_specs if s.constraint != MonotonicityConstraint.NONE)
            compliant = len(training_result.monotonic_features)
            violations = len(training_result.non_monotonic_violations)
            compliance_rate = compliant / total_constrained * 100 if total_constrained > 0 else 100.0

            md += f"""**Compliance Rate:** {compliance_rate:.1f}%

- Constrained Features: {total_constrained}
- Compliant: {compliant}
- Violations: {violations}

"""

            if training_result.non_monotonic_violations:
                md += "### Violated Constraints\n\n"
                for feat in training_result.non_monotonic_violations:
                    md += f"- `{feat}`\n"
        else:
            md += "_Monotonicity compliance data not available._\n"

        # Drift Distribution (if scores provided)
        if drift_scores:
            md += """
---

## 5. Drift Score Distribution

"""
            scores = [s.drift_score for s in drift_scores]

            md += f"""| Statistic | Value |
|-----------|-------|
| Count | {len(scores):,} |
| Mean | {np.mean(scores):.4f} |
| Std | {np.std(scores):.4f} |
| Min | {np.min(scores):.4f} |
| Max | {np.max(scores):.4f} |
| P50 | {np.percentile(scores, 50):.4f} |
| P95 | {np.percentile(scores, 95):.4f} |

### Severity Distribution

"""
            severity_counts = defaultdict(int)
            for s in drift_scores:
                severity_counts[s.severity.value] += 1

            md += "| Severity | Count | Percentage |\n"
            md += "|----------|-------|------------|\n"
            for sev in EBMDriftSeverity:
                count = severity_counts.get(sev.value, 0)
                pct = count / len(drift_scores) * 100 if drift_scores else 0
                md += f"| {sev.value} | {count:,} | {pct:.1f}% |\n"

        # Recommendations
        md += """
---

## 6. Recommendations

"""
        recommendations = self._generate_recommendations(training_result, drift_scores)
        for i, rec in enumerate(recommendations, 1):
            md += f"{i}. {rec}\n"

        # Appendix
        md += """
---

## Appendix A: EBM Configuration

```yaml
"""
        for key, value in EBM_DEFAULT_CONFIG.items():
            md += f"{key}: {value}\n"

        md += """```

## Appendix B: Drift Severity Thresholds

| Severity | Threshold |
|----------|-----------|
"""
        for sev, thresh in EBM_DRIFT_THRESHOLDS.items():
            md += f"| {sev.upper()} | â‰¤{thresh:.0%} |\n"

        md += """
---

ðŸ©· **MAGGIE â€” GID-10 â€” MACHINE LEARNING LEAD**
*Glass-box models for transparent decisions.*
ðŸ©·ðŸ©·ðŸ©· END OF REPORT ðŸ©·ðŸ©·ðŸ©·
"""

        return md

    def _generate_recommendations(
        self,
        training_result: EBMTrainingResult | None,
        drift_scores: List[EBMDriftScore] | None,
    ) -> List[str]:
        """Generate actionable recommendations."""
        recommendations = []

        if training_result:
            # AUC-based recommendations
            if training_result.auc_validation < 0.7:
                recommendations.append("ðŸ”´ **LOW AUC:** Model AUC < 0.7. Consider additional features or hyperparameter tuning.")
            elif training_result.auc_validation < 0.8:
                recommendations.append("ðŸŸ¡ **MODERATE AUC:** Model AUC < 0.8. Review feature engineering opportunities.")
            else:
                recommendations.append("ðŸŸ¢ **GOOD AUC:** Model AUC â‰¥ 0.8. Performance is acceptable.")

            # Calibration
            if training_result.calibration_error > 0.1:
                recommendations.append("âš ï¸ **CALIBRATION ISSUE:** ECE > 0.1. Consider Platt scaling or isotonic regression.")

            # Monotonicity
            if training_result.non_monotonic_violations:
                recommendations.append(
                    f"âš ï¸ **MONOTONICITY VIOLATIONS:** {len(training_result.non_monotonic_violations)} features "
                    "violate constraints. Review feature transforms or increase constraint strength."
                )

        if drift_scores:
            scores = [s.drift_score for s in drift_scores]
            p95 = np.percentile(scores, 95)

            if p95 > 0.4:
                recommendations.append("ðŸ”´ **HIGH DRIFT:** P95 drift score > 0.4. Investigate top contributing features.")
            elif p95 > 0.25:
                recommendations.append("ðŸŸ¡ **ELEVATED DRIFT:** P95 drift score > 0.25. Monitor and schedule review.")

        if not recommendations:
            recommendations.append("ðŸŸ¢ **HEALTHY:** No significant issues detected.")

        return recommendations

    def save_report(
        self,
        report: str,
        path: Path | str | None = None,
    ) -> Path:
        """Save report to file."""
        if path is None:
            path = Path("docs/ml/EBM_DRIFT_V3.md")
        else:
            path = Path(path)

        path.parent.mkdir(parents=True, exist_ok=True)
        path.write_text(report, encoding="utf-8")

        logger.info("Saved EBM drift report to %s", path)
        return path


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# CONVENIENCE FUNCTIONS
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


def train_ebm_drift_model(
    X: np.ndarray,
    y: np.ndarray,
    feature_names: List[str],
    config: Dict[str, Any] | None = None,
    validation_split: float = 0.15,
    save_model: bool = True,
) -> Tuple[EBMDriftModel, EBMTrainingResult]:
    """
    Train EBM drift model with default configuration.

    Args:
        X: Training features
        y: Binary labels
        feature_names: Feature names
        config: Optional custom config
        validation_split: Validation split ratio
        save_model: Whether to save model to disk

    Returns:
        (trained_model, training_result)
    """
    print("\n" + "=" * 60)
    print("ðŸ©· MAGGIE EBM DRIFT MODEL TRAINING v3")
    print("=" * 60)

    # Split data
    n_samples = len(y)
    n_val = int(n_samples * validation_split)

    indices = np.random.permutation(n_samples)
    train_idx, val_idx = indices[n_val:], indices[:n_val]

    X_train, y_train = X[train_idx], y[train_idx]
    X_val, y_val = X[val_idx], y[val_idx]

    print(f"\n[1/4] Data split: {len(y_train)} train, {len(y_val)} validation")

    # Build constraints
    print("\n[2/4] Building monotonicity constraints...")
    builder = MonotonicityConstraintBuilder()
    constraints = builder.build_constraints(feature_names)

    increasing = sum(1 for c in constraints if c.constraint == MonotonicityConstraint.INCREASING)
    decreasing = sum(1 for c in constraints if c.constraint == MonotonicityConstraint.DECREASING)
    print(f"   - Increasing: {increasing}, Decreasing: {decreasing}")

    # Train model
    print("\n[3/4] Training EBM model...")
    model = EBMDriftModel(config=config, feature_names=feature_names)
    result = model.fit(
        X_train,
        y_train,
        feature_names=feature_names,
        monotonicity_constraints=constraints,
        validation_data=(X_val, y_val),
    )

    print(f"   - Train AUC: {result.auc_train:.4f}")
    print(f"   - Val AUC: {result.auc_validation:.4f}")
    print(f"   - Calibration Error: {result.calibration_error:.4f}")
    print(f"   - Monotonicity Violations: {len(result.non_monotonic_violations)}")

    # Save model
    if save_model:
        print("\n[4/4] Saving model...")
        model.save()
    else:
        print("\n[4/4] Skipping model save")

    print("\n" + "=" * 60)
    print("âœ“ EBM TRAINING COMPLETE")
    print("=" * 60)

    return model, result


def generate_ebm_drift_report(
    model: EBMDriftModel,
    training_result: EBMTrainingResult | None = None,
    drift_scores: List[EBMDriftScore] | None = None,
    output_path: Path | str | None = None,
) -> str:
    """
    Generate and save EBM drift report.

    Args:
        model: Trained EBM model
        training_result: Optional training results
        drift_scores: Optional drift scores for analysis
        output_path: Custom output path

    Returns:
        Report markdown content
    """
    generator = EBMDriftReportGenerator(model)
    report = generator.generate_report(training_result, drift_scores)

    if output_path:
        generator.save_report(report, output_path)

    return report


# Export public API
__all__ = [
    # Enums
    "EBMDriftSeverity",
    "MonotonicityConstraint",
    "InteractionType",
    # Data classes
    "MonotonicitySpec",
    "EBMFeatureShape",
    "EBMInteraction",
    "EBMAttribution",
    "EBMDriftScore",
    "EBMTrainingResult",
    "EBMDriftReport",
    # Classes
    "MonotonicityConstraintBuilder",
    "EBMDriftModel",
    "EBMFallbackScorer",
    "EBMDriftReportGenerator",
    # Functions
    "train_ebm_drift_model",
    "generate_ebm_drift_report",
    # Constants
    "EBM_DEFAULT_CONFIG",
    "EBM_DRIFT_THRESHOLDS",
]
